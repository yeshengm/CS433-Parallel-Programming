\documentclass{article}
\usepackage{listings}
\usepackage{color}
\usepackage{float}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
	frame=single,
	language=C,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	captionpos=b,
	basicstyle={\small\ttfamily},
	numbers=left,
	numbersep=5pt,
	%numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4
}

\input kvmacros
\sloppy
\begin{document}
\title{Programming Assignment 1- Parallel Dijkstra}
\author{\textit{Ke Chang}\\\textit{Yesheng Ma}}
\date{Oct 20th, 2016}
\maketitle


\begin{abstract}
	The known Dijkstra algorithm is used to find the single-source shortest path problem. And what we have done in this project is to parallelize it in order to complete the task with higher efficiency.
\end{abstract}


\section{Introduction}
First let's shed a light on how serial Dijkstra algorithm work. The main body is n-1 iterations. In each iteration, the algorithm selects the vertex which has the smallest distance from the origin vertex among those unknown vertices. Then it will be put in the known list and all its adjacent vertices will be updated for next iteration.\\
The general idea behind the partition is not to completely separate the whole algorithm, which will be too complex, but to decompose one single step- to find the min distance among the remaining vertex. During this process, we can divide the whole vertex set into n parts. And each progress finds the one with smallest distance respectively. And progress 0 does the reduce work- to find the desired one among those chosen one.  

\section{Implementation}
\subsection{Build and Test Automation}
During building the whole project, we use the tool GUN Makefile, which is a software for auto-build of C programs. Also we write a shell script to test the program, which takes two arguments, i.e. data size and thread number. Example usage can be \verb|sh test.sh 1000 10|.
\subsection{Implementation of Parallel Dijkstra Algorithm}
\subsubsection{MPI API}
We use several MPI APIs to implement this algorithm, type signatures are left over for simplicity:
\begin{itemize}
	\item \verb|MPI_Init| mark the entry of MPI code
	\item \verb|MPI_Comm_size| get the size of MPI communication space
	\item \verb|MPI_Comm_rank| get the rank of current thread
	\item \verb|MPI_Allreduce| similar to the reduce function in functional programming languages
	\item \verb|MPI_Gather| gather arrays in different threads to form a large array
\end{itemize}
\subsubsection{Core implementation}
The code for parallel Dijkstra is mainly shown in the \verb|Dijkstra| function shown in source code. Before execution of this function, we have already partitioned the input data into separate matrices \verb|loc_mat|. All the executions of \verb|Dijkstra| should be make on these matrices.

Next, we will explain how this \verb|Dijkstra| function actually works. In fact, the parallel Dijkstra is a quite naive algorithm. The major part parallelized is computation of the node which is currently unmarked and has the minimal distance to source. To operate on one submatrix, we first need to declare an array \verb|loc_known| denoting the currently marked vertices. In next n-1 iterations, we find the vertices with locally minimal distance and reduce this to get the vertex with global minimal distance. Finally, we use this new minimal-distance vertex to update all unmarked vertices. Code for this algorithm is shown below:
\begin{lstlisting}
void Dijkstra(int loc_mat[], int loc_dist[], int loc_pred[], int loc_n, int my_rank, int n)
{
	int loc_v, *loc_known;
	loc_known = malloc(loc_n * sizeof(int));
	for (loc_v = 0; loc_v < loc_n; loc_v++) {
		loc_dist[loc_v] = loc_mat[0*loc_n + loc_v];
		loc_known[loc_v] = 0;
		loc_pred[loc_v] = 0;
	}
	if (my_rank == 0)
	loc_known[0] = 1;
	for (int j = 1; j < n; j++) {
		int my_min[2], glbl_min[2];
		Find_min_loc_dist(loc_dist, loc_known, loc_n, my_min, my_rank);
		MPI_Allreduce(my_min, glbl_min, 1, MPI_2INT, MPI_MINLOC, MPI_COMM_WORLD);
		if (my_rank == glbl_min[1]/loc_n) {
			loc_known[glbl_min[1]%loc_n] = 1;
		}
		int new_loc_dist;
		for (loc_v = 0; loc_v < loc_n; loc_v++) {
			if (!loc_known[loc_v]) {
			new_loc_dist = glbl_min[0] + loc_mat[glbl_min[1]*loc_n + loc_v];	
				if (new_loc_dist < loc_dist[loc_v]) {
					loc_dist[loc_v] = new_loc_dist;
					loc_pred[loc_v] = glbl_min[1];
				}
			}
		}
	}
	free(loc_known);
}
\end{lstlisting}
\subsubsection{Potential Risks}
During programming this algorithm, I found that if the graph is complex and edge weight is large, we may suffer from integer overflow. However, this may not be a big issue for data set in this course. To handle this issue, we may introduce a high-precision integer class, which can lead to some overhead.



\subsection{Run It on Cluster}
After the completion of the whole algortihm, we started to deploy it on the cluster. We utilize \textit{scp} command to transfer files to the cluster. Then log in via \textit{ssh}. After these pre-works, we write the slurm script to submmit the job. The script is shown below:
\begin{lstlisting}
#!/bin/bash
#SBATCH --job-name=mpi_io
#SBATCH --partition=cpu
#SBATCH --mail-type=end
#SBATCH --mail-user=cktonychang@gmail.com
#SBATCH --ntask-per-node=16
source /usr/share/Modules/init/bash
unset MODULEPATH
module use /lustre/usr/modulefiles/pi
module purge
module load gcc openmpic

srun -n $1 --mpi=pmi2 --error=./result/$1_$2.err --output=./result/$1_$2.txt ./mpi_io ./data/$2.txt
\end{lstlisting}
Then we run all input files in the test set on 1, 2, 4, 8 progresses respectively and modify the code to record the serial and parallel time.
\section{Result \& Analysis}
\begin{center}
 \begin{table}
 \begin{tabular}[t]{r|ccccccccccc}
$\# proc$&10&100&300&400&500&800&1000&2000&3000&4000&5000\\
\hline
1&140&130&150&170&170&240&250&600&1290&2210&2950\\
2&150&170&180&210&210&270&320&730&1290&1870&2880\\
4&170&160&170&180&190&230&270&710&1280&2030&2910\\
8&210&170&260&220&250&270&310&710&1170&1990&2870\\
\end{tabular}
\caption{Serial Time}
\begin{tabular}[t]{r|ccccccccccc}
$\# proc$&10&100&300&400&500&800&1000&2000&3000&4000&5000\\
\hline
1&0&0&10&0&0&10&10&50&90&150&260\\
2&0&0&0&0.&0&10&0&30&40&90&140\\
4&0&0&0&0&0&10&0&20&40&60&90\\
8&10&0&0&0&0&0&10&10&20&40&50\\
\end{tabular}
\caption{parallel Time}
\end{table}
\end{center}
From these two tables we can see some important facts: Most of the serial time is used on IO instead of the paralle algorithm. Although the number of progresses increased, the total time consumption stays almost the same. The reason besides IO time may be that most work is done by progress 0. So the speed-up is 1 and the efficiency is very low. All of these reflect the imperfection in the algorithm itself.
\end{document}
